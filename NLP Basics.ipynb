{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Basics: What is Natural Language Processing & the Natural Language Toolkit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to install NLTK on your local machine\n",
    "\n",
    "Both sets of instructions below assume you already have Python installed. These instructions are taken directly from [http://www.nltk.org/install.html](http://www.nltk.org/install.html).\n",
    "\n",
    "**Mac/Unix**\n",
    "\n",
    "From the terminal:\n",
    "1. Install NLTK: run `pip install -U nltk`\n",
    "2. Test installation: run `python` then type `import nltk`\n",
    "\n",
    "**Windows**\n",
    "\n",
    "1. Install NLTK: [http://pypi.python.org/pypi/nltk](http://pypi.python.org/pypi/nltk)\n",
    "2. Test installation: `Start>Python35`, then type `import nltk`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NLTK data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AbstractLazySequence',\n",
       " 'AffixTagger',\n",
       " 'AlignedSent',\n",
       " 'Alignment',\n",
       " 'AnnotationTask',\n",
       " 'ApplicationExpression',\n",
       " 'Assignment',\n",
       " 'BigramAssocMeasures',\n",
       " 'BigramCollocationFinder',\n",
       " 'BigramTagger',\n",
       " 'BinaryMaxentFeatureEncoding',\n",
       " 'BlanklineTokenizer',\n",
       " 'BllipParser',\n",
       " 'BottomUpChartParser',\n",
       " 'BottomUpLeftCornerChartParser',\n",
       " 'BottomUpProbabilisticChartParser',\n",
       " 'Boxer',\n",
       " 'BrillTagger',\n",
       " 'BrillTaggerTrainer',\n",
       " 'CFG']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nltk)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can you do with NLTK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " \"you've\",\n",
       " 'himself',\n",
       " 'they',\n",
       " 'that',\n",
       " 'been',\n",
       " 'a',\n",
       " 'while',\n",
       " 'through',\n",
       " 'in',\n",
       " 'here',\n",
       " 'few',\n",
       " 'own',\n",
       " 'just',\n",
       " 're',\n",
       " 'doesn',\n",
       " 'ma',\n",
       " \"shouldn't\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[0:300:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in text data & why do we need to clean the text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in semi-structured text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ham\\tI've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\\nspam\\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receiv\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the raw text\n",
    "rawData = open(\"SMSSpamCollection.tsv\").read()\n",
    "\n",
    "# Print the raw data\n",
    "rawData[0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with \\t and \\n\n",
    "parsedData = rawData.replace('\\t','\\n').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham',\n",
       " \"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\",\n",
       " 'spam',\n",
       " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       " 'ham',\n",
       " \"Nah I don't think he goes to usf, he lives around here though\",\n",
       " 'ham',\n",
       " 'Even my brother is not like to speak with me. They treat me like aids patent.',\n",
       " 'ham',\n",
       " 'I HAVE A DATE ON SUNDAY WITH WILL!!']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedData[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'spam', 'ham', 'ham', 'ham']\n",
      "[\"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\", \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\", \"Nah I don't think he goes to usf, he lives around here though\", 'Even my brother is not like to speak with me. They treat me like aids patent.', 'I HAVE A DATE ON SUNDAY WITH WILL!!']\n"
     ]
    }
   ],
   "source": [
    "#fetching label and body data in seperate lists\n",
    "labelList = parsedData[0::2]\n",
    "textList = parsedData[1::2]\n",
    "print(labelList[0:5])\n",
    "print(textList[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5571\n",
      "5570\n"
     ]
    }
   ],
   "source": [
    "# to create pandas dictionary the length of label and body should be same\n",
    "print(len(labelList))\n",
    "print(len(textList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'ham', 'ham', 'ham', '']\n"
     ]
    }
   ],
   "source": [
    "#mistmatch between label and body, lets find out where\n",
    "print(labelList[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          body_list\n",
       "0   ham  I've been searching for the right words to tha...\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3   ham  Even my brother is not like to speak with me. ...\n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fullCorpus = pd.DataFrame({\n",
    "    'label' : labelList[:-1], # ignoring the last element\n",
    "    'body_list' : textList\n",
    "})\n",
    "fullCorpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is tab limited, we can directly read it from read.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          body_text\n",
       "0   ham  I've been searching for the right words to tha...\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3   ham  Even my brother is not like to speak with me. ...\n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullCorpus = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t', header =None)\n",
    "fullCorpus.columns = ['label', 'body_text']\n",
    "fullCorpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 5568 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset has {} rows and {} columns\".format(len(fullCorpus), len(fullCorpus.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam count is 746\n",
      "ham count is 4822\n"
     ]
    }
   ],
   "source": [
    "print(\"spam count is {}\".format(len(fullCorpus[fullCorpus['label']=='spam'])))\n",
    "print(\"ham count is {}\".format(len(fullCorpus[fullCorpus['label']=='ham'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label        0\n",
       "body_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullCorpus.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a pipeline to clean text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing text data\n",
    "\n",
    "Cleaning up the text data is necessary to highlight attributes that you're going to want your machine learning system to pick up on. Cleaning (or pre-processing) the data typically consists of a number of steps:\n",
    "1. **Remove punctuation**\n",
    "2. **Tokenization**\n",
    "3. **Remove stopwords**\n",
    "4. Lemmatize/Stem\n",
    "\n",
    "The first three steps are covered in this chapter as they're implemented in pretty much any text cleaning pipeline. Lemmatizing and stemming are covered in the next chapter as they're helpful but not critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...  \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though  \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.  \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t', header=None)\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                       body_text_clean  \n",
       "0  Ive been searching for the right words to thank you for this breather I promise i wont take your...  \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...  \n",
       "2                                          Nah I dont think he goes to usf he lives around here though  \n",
       "3                          Even my brother is not like to speak with me They treat me like aids patent  \n",
       "4                                                                    I HAVE A DATE ON SUNDAY WITH WILL  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punc(text):\n",
    "    text_nonpunct = ''.join([char for char in text if char not in string.punctuation]) \n",
    "    # list compression : \"char for char in text if char not in string.punctuation\" will return single characters in the list hence grouping required.\n",
    "    return text_nonpunct\n",
    "\n",
    "data['body_text_clean'] = data['body_text'].apply(lambda x:remove_punc(x))\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                       body_text_clean  \\\n",
       "0  Ive been searching for the right words to thank you for this breather I promise i wont take your...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "2                                          Nah I dont think he goes to usf he lives around here though   \n",
       "3                          Even my brother is not like to speak with me They treat me like aids patent   \n",
       "4                                                                    I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                                                                   body_text_tokenized  \n",
       "0  [ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...  \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...  \n",
       "2                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]  \n",
       "3         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]  \n",
       "4                                                           [i, have, a, date, on, sunday, with, will]  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "data['body_text_tokenized'] = data['body_text_clean'].apply(lambda x: tokenize(x.lower()))\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_clean</th>\n",
       "      <th>body_text_tokenized</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>Ive been searching for the right words to thank you for this breather I promise i wont take your...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>Even my brother is not like to speak with me They treat me like aids patent</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                       body_text_clean  \\\n",
       "0  Ive been searching for the right words to thank you for this breather I promise i wont take your...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "2                                          Nah I dont think he goes to usf he lives around here though   \n",
       "3                          Even my brother is not like to speak with me They treat me like aids patent   \n",
       "4                                                                    I HAVE A DATE ON SUNDAY WITH WILL   \n",
       "\n",
       "                                                                                   body_text_tokenized  \\\n",
       "0  [ive, been, searching, for, the, right, words, to, thank, you, for, this, breather, i, promise, ...   \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...   \n",
       "2                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "3         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]   \n",
       "4                                                           [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                                                                                                 clean  \n",
       "0  [ive, searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, prom...  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "2                                                 [nah, dont, think, goes, usf, lives, around, though]  \n",
       "3                                              [even, brother, like, speak, treat, like, aids, patent]  \n",
       "4                                                                                       [date, sunday]  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "data['clean'] = data['body_text_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning : Using Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MARTIN_EXTENSIONS',\n",
       " 'NLTK_EXTENSIONS',\n",
       " 'ORIGINAL_ALGORITHM',\n",
       " '__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(ps)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean\n",
      "clean\n",
      "clean\n",
      "cleaner\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('clean'))\n",
    "print(ps.stem('cleaning'))\n",
    "print(ps.stem('cleaned'))\n",
    "print(ps.stem('cleaner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...  \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though  \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.  \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t', header=None)\n",
    "data.columns = ['label', 'body_text']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nonstop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>[searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, promise, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[date, sunday, ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                     body_text_nonstop  \n",
       "0  [searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, promise, ...  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "2                                                       [nah, think, goes, usf, lives, around, though]  \n",
       "3                                            [even, brother, like, speak, treat, like, aids, patent, ]  \n",
       "4                                                                                     [date, sunday, ]  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text_nonpunct = ''.join([char for char in text if char not in string.punctuation]) \n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "data['body_text_nonstop'] = data['body_text'].apply(lambda x: clean_text(x.lower()))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nonstop</th>\n",
       "      <th>body_text_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>[searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, promise, ...</td>\n",
       "      <td>[search, right, word, thank, breather, promis, wont, take, help, grant, fulfil, promis, wonder, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent, ]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[date, sunday, ]</td>\n",
       "      <td>[date, sunday, ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                     body_text_nonstop  \\\n",
       "0  [searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, promise, ...   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "2                                                       [nah, think, goes, usf, lives, around, though]   \n",
       "3                                            [even, brother, like, speak, treat, like, aids, patent, ]   \n",
       "4                                                                                     [date, sunday, ]   \n",
       "\n",
       "                                                                                        body_text_stem  \n",
       "0  [search, right, word, thank, breather, promis, wont, take, help, grant, fulfil, promis, wonder, ...  \n",
       "1  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...  \n",
       "2                                                         [nah, think, goe, usf, live, around, though]  \n",
       "3                                             [even, brother, like, speak, treat, like, aid, patent, ]  \n",
       "4                                                                                     [date, sunday, ]  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stemming(tokenized_text):\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "data['body_text_stem'] = data['body_text_nonstop'].apply(lambda x : stemming(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning : Using Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "wn = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(wn)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_nonstop</th>\n",
       "      <th>body_text_stem</th>\n",
       "      <th>body_text_lemmatize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>[searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, promise, ...</td>\n",
       "      <td>[search, right, word, thank, breather, promis, wont, take, help, grant, fulfil, promis, wonder, ...</td>\n",
       "      <td>[searching, right, word, thank, breather, promise, wont, take, help, granted, fulfil, promise, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, think, goe, usf, live, around, though]</td>\n",
       "      <td>[nah, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids, patent, ]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent, ]</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid, patent, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[date, sunday, ]</td>\n",
       "      <td>[date, sunday, ]</td>\n",
       "      <td>[date, sunday, ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                     body_text_nonstop  \\\n",
       "0  [searching, right, words, thank, breather, promise, wont, take, help, granted, fulfil, promise, ...   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "2                                                       [nah, think, goes, usf, lives, around, though]   \n",
       "3                                            [even, brother, like, speak, treat, like, aids, patent, ]   \n",
       "4                                                                                     [date, sunday, ]   \n",
       "\n",
       "                                                                                        body_text_stem  \\\n",
       "0  [search, right, word, thank, breather, promis, wont, take, help, grant, fulfil, promis, wonder, ...   \n",
       "1  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...   \n",
       "2                                                         [nah, think, goe, usf, live, around, though]   \n",
       "3                                             [even, brother, like, speak, treat, like, aid, patent, ]   \n",
       "4                                                                                     [date, sunday, ]   \n",
       "\n",
       "                                                                                   body_text_lemmatize  \n",
       "0  [searching, right, word, thank, breather, promise, wont, take, help, granted, fulfil, promise, w...  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "2                                                          [nah, think, go, usf, life, around, though]  \n",
       "3                                             [even, brother, like, speak, treat, like, aid, patent, ]  \n",
       "4                                                                                     [date, sunday, ]  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "data['body_text_lemmatize'] = data['body_text_nonstop'].apply(lambda x : lemmatize(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing Raw Data: Count Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count vectorization \n",
    "\n",
    "Creates a document-term matrix where the entry of each cell will be a count of the number of times that word occurred in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create function to remove punctuation, tokenize, remove stopwords, and stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5567, 10775)\n",
      "['', '0', '00', '000', '000pes', '008704050406', '0089', '0121', '01223585236', '01223585334', '0125698789', '02', '0207', '02072069400', '02073162414', '02085076972', '021', '03', '04', '0430', '05', '050703', '0578', '06', '07', '07008009200', '07046744435', '07090201529', '07090298926', '07099833605', '07123456789', '0721072', '07732584351', '07734396839', '07742676969', '07753741225', '0776xxxxxxx', '07781482378', '07786200117', '077xxx', '078', '07801543489', '07808', '07808247860', '07808726822', '07815296484', '07821230901', '078498', '07880867867', '0789xxxxxxx', '07946746291', '0796XXXXXX', '07973788240', '07XXXXXXXXX', '07xxxxxxxxx', '08', '0800', '08000407165', '08000776320', '08000839402', '08000930705', '08000938767', '08001950382', '08002888812', '08002986030', '08002986906', '08002988890', '08006344447', '0808', '08081263000', '08081560665', '0825', '083', '0844', '08448350055', '08448714184', '0845', '08450542832', '08452810071', '08452810073', '08452810075over18', '0870', '08700435505150p', '08700469649', '08700621170150p', '08701213186', '08701237397', '08701417012', '08701417012150p', '0870141701216', '087016248', '08701752560', '087018728737', '0870241182716', '08702490080', '08702840625', '08704050406', '08704439680', '08704439680Ts', '08706091795']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(analyzer = clean_text)\n",
    "x_counts = count_vect.fit_transform(data['body_text'])\n",
    "print(x_counts.shape)\n",
    "print(count_vect.get_feature_names()[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply CountVectorizer to smaller sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 232)\n",
      "['', '000', '08002986030', '08452810075over18', '09061701461', '1', '100', '11', '12', '150p', '16', '2', '20', '2005', '21st', '3', '4', '4403LDNW1A7RW18', '4txt', '6days', '81010', '87077', '87121', '87575', '9', '900', 'A', 'Aft', 'Alright', 'Ard', 'As', 'C', 'CASH', 'CLAIM', 'CSH11', 'Call', 'Callers', 'Callertune', 'Claim', 'Co', 'Cost', 'Cup', 'DATE', 'ENGLAND', 'Eh', 'England', 'Even', 'FA', 'FREE', 'Ffffffffff', 'Fine', 'Free', 'From', 'HAVE', 'HL', 'Had', 'He', 'I', 'Is', 'Jackpot', 'KL341', 'LCCLTD', 'Macedonia', 'May', 'Melle', 'Minnaminunginte', 'Mobile', 'Nah', 'No', 'Nurungu', 'ON', 'Oh', 'Oru', 'POBOX', 'POBOXox36504W45WQ', 'Press', 'Prize', 'QJKGIGHJJGCBL', 'R', 'Reply', 'SCOTLAND', 'SIX', 'SUNDAY', 'So', 'T', 'Text', 'That', 'The', 'Then', 'They', 'To', 'Try', 'TsandCs', 'Txt', 'U', 'URGENT', 'Update', 'Valid', 'Vettam', 'WALES', 'WAP', 'WILL', 'WINNER', 'WITH', 'XXXMobileMovieClub', 'Yes', 'You', 'aids', 'already', 'anymore', 'apply', 'around', 'b', 'brother', 'call', 'callertune', 'camera', 'chances', 'claim', 'click', 'code', 'colour', 'com', 'comin', 'comp', 'copy', 'credit', 'cried', 'customer', 'da', 'day', 'dbuk', 'dont', 'eg', 'enough', 'entitled', 'entry', 'feel', 'final', 'finish', 'first', 'friends', 'go', 'goals', 'goes', 'going', 'gonna', 'gota', 'ha', 'home', 'hours', 'http', 'info', 'joking', 'k', 'lar', 'latest', 'like', 'link', 'lives', 'lor', 'lunch', 'make', 'meet', 'membership', 'message', 'miss', 'mobile', 'mobiles', 'months', 'n', 'name', 'national', 'naughty', 'net', 'network', 'news', 'next', 'patent', 'pay', 'per', 'pounds', 'prize', 'question', 'rate', 'receive', 'receivea', 'remember', 'request', 'reward', 'selected', 'send', 'seriously', 'set', 'smth', 'soon', 'sooner', 'speak', 'spell', 'std', 'stock', 'str', 'stuff', 'talk', 'team', 'think', 'though', 'tkts', 'today', 'tonight', 'treat', 'try', 'txt', 'u', 'ur', 'use', 'usf', 'v', 'valued', 'want', 'wap', 'watching', 'way', 'week', 'wet', 'win', 'wkly', 'word', 'www', 'xxxmobilemovieclub', 'ú1', 'ü']\n"
     ]
    }
   ],
   "source": [
    "data_sample = data[0:20]\n",
    "\n",
    "count_vect_sample = CountVectorizer(analyzer = clean_text)\n",
    "x_counts_sample = count_vect_sample.fit_transform(data_sample['body_text'])\n",
    "print(x_counts_sample.shape)\n",
    "print(count_vect_sample.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizers output sparse matrices\n",
    "\n",
    "_**Sparse Matrix**: A matrix in which most entries are 0. In the interest of efficient storage, a sparse matrix will be stored by only storing the locations of the non-zero elements._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20x232 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 272 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_counts_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 232 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  222  223  224  225  \\\n",
       "0    0    0    0    1    0    0    0    0    0    0  ...    0    0    0    1   \n",
       "1    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "2    1    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "3    1    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "4    0    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "\n",
       "   226  227  228  229  230  231  \n",
       "0    1    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 232 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Document term matrix\n",
    "x_count_df = pd.DataFrame(x_counts_sample.toarray())\n",
    "x_count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>08002986030</th>\n",
       "      <th>08452810075over18</th>\n",
       "      <th>09061701461</th>\n",
       "      <th>1</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>150p</th>\n",
       "      <th>...</th>\n",
       "      <th>way</th>\n",
       "      <th>week</th>\n",
       "      <th>wet</th>\n",
       "      <th>win</th>\n",
       "      <th>wkly</th>\n",
       "      <th>word</th>\n",
       "      <th>www</th>\n",
       "      <th>xxxmobilemovieclub</th>\n",
       "      <th>ú1</th>\n",
       "      <th>ü</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 232 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      000  08002986030  08452810075over18  09061701461  1  100  11  12  150p  \\\n",
       "0  0    0            0                  1            0  0    0   0   0     0   \n",
       "1  0    0            0                  0            0  0    0   0   0     0   \n",
       "2  1    0            0                  0            0  0    0   0   0     0   \n",
       "3  1    0            0                  0            0  0    0   0   0     0   \n",
       "4  0    0            0                  0            0  0    0   0   0     0   \n",
       "\n",
       "   ...  way  week  wet  win  wkly  word  www  xxxmobilemovieclub  ú1  ü  \n",
       "0  ...    0     0    0    1     1     0    0                   0   0  0  \n",
       "1  ...    0     0    0    0     0     0    0                   0   0  0  \n",
       "2  ...    0     0    0    0     0     0    0                   0   0  0  \n",
       "3  ...    0     0    0    0     0     0    0                   0   0  0  \n",
       "4  ...    0     0    0    0     0     0    0                   0   0  0  \n",
       "\n",
       "[5 rows x 232 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_count_df.columns = count_vect_sample.get_feature_names()\n",
    "x_count_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing Raw Data: N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams \n",
    "\n",
    "Creates a document-term matrix where counts still occupy the cell but instead of the columns representing single terms, they represent all combinations of adjacent words of length n in your text.\n",
    "\n",
    "\"NLP is an interesting topic\"\n",
    "\n",
    "| n | Name      | Tokens                                                         |\n",
    "|---|-----------|----------------------------------------------------------------|\n",
    "| 2 | bigram    | [\"nlp is\", \"is an\", \"an interesting\", \"interesting topic\"]      |\n",
    "| 3 | trigram   | [\"nlp is an\", \"is an interesting\", \"an interesting topic\"] |\n",
    "| 4 | four-gram | [\"nlp is an interesting\", \"is an interesting topic\"]    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create function to remove punctuation, tokenize, remove stopwords, and stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri questionstd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>nah dont think goe usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>even brother like speak treat like aid patent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>date sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...</td>\n",
       "      <td>per request mell mell oru minnaminungint nurungu vettam set callertun caller press 9 copi friend...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0  spam   \n",
       "1   ham   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "1                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "2                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "3                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "4  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your call...   \n",
       "\n",
       "                                                                                          cleaned_text  \n",
       "0  free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri questionstd...  \n",
       "1                                                            nah dont think goe usf live around though  \n",
       "2                                                        even brother like speak treat like aid patent  \n",
       "3                                                                                          date sunday  \n",
       "4  per request mell mell oru minnaminungint nurungu vettam set callertun caller press 9 copi friend...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = \" \".join([ps.stem(word) for word in tokens if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "data['cleaned_text'] = data['body_text'].apply(lambda x: clean_text(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply CountVectorizer (w/ N-Grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5567, 31260)\n",
      "['008704050406 sp', '0089mi last', '0121 2025050', '01223585236 xx', '01223585334 cum', '0125698789 ring', '02 user', '020603 2nd', '0207 153', '02072069400 bx', '02073162414 cost', '02085076972 repli', '020903 2nd', '021 3680', '021 3680offer', '050703 tcsbcm4235wc1n3xx', '06 good', '07046744435 arrang', '07090298926 reschedul', '07099833605 reschedul', '07123456789 87077', '0721072 find', '07732584351 rodger', '07734396839 ibh', '07742676969 show', '07753741225 show', '0776xxxxxxx uve', '077xxx 2000', '07801543489 guarante', '07808 xxxxxx', '07808247860 show', '07808726822 award', '07815296484 show', '0784987 show', '0789xxxxxxx today', '0796xxxxxx today', '07973788240 show', '07xxxxxxxxx 2000', '07xxxxxxxxx show', '0800 0721072', '0800 169', '0800 18', '0800 195', '0800 1956669', '0800 505060', '0800 542', '08000407165 18', '08000776320 repli', '08000839402 2stoptx', '08000839402 2stoptxt', '08000839402 call', '08000839402 call2optout4qf2', '08000839402 call2optoutf4q', '08000839402 call2optouthf8', '08000839402 call2optoutj5q', '08000839402 call2optoutlf56', '08000839402 call2optoutn9dx', '08000839402 call2optoutyhl', '08000839402 or2optouthv9d', '08000930705 del', '08000930705 deliveri', '08000930705 free', '08000930705 repli', '08000938767 updat', '08001950382 call', '08001950382 call2optout674', '08001950382 call2optoutj', '08002888812 repli', '08002988890 repli', '08006344447 claim', '0808 145', '08081263000 charg', '08081560665 speak', '0844 861', '08448350055 bt', '08448714184 stoptxt', '0845 021', '0845 2814032', '08452810071 16', '08452810073 detail', '0870 chatlin', '0870 nation', '08700621170150p per', '08701237397 must', '08701417012 profit', '08701417012150p per', '0870141701216 norm', '08701752560 450p', '087018728737 nation', '0870241182716 3wk', '08702490080 tell', '08702840625 comuk220cm2', '08702840625comuk 220cm2', '08704439680 book', '08704439680tsc appli', '0870737910216yr 150wk', '08707500020 20p', '08707509020 20p', '08708800282 hgsuite3422land', '08709222922 nation']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "ngram_vect = CountVectorizer(ngram_range = (2,2))\n",
    "x_count = ngram_vect.fit_transform(data['cleaned_text'])\n",
    "print(x_count.shape)\n",
    "print(ngram_vect.get_feature_names()[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply CountVectorizer (w/ N-Grams) to smaller sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 198)\n",
      "['09061701461 claim', '100 20000', '100000 prize', '11 month', '12 hour', '150pday 6day', '16 tsandc', '20000 pound', '2005 text', '21st may', '4txtú120 poboxox36504w45wq', '6day 16', '81010 tc', '87077 eg', '87077 trywal', '87121 receiv', '87575 cost', '900 prize', 'aft finish', 'aid patent', 'alright way', 'anymor tonight', 'appli 08452810075over18', 'appli repli', 'ard smth', 'around though', 'brother like', 'call 09061701461', 'call mobil', 'caller press', 'callertun caller', 'camera free', 'cash 100', 'chanc win', 'claim 81010', 'claim call', 'claim code', 'click httpwap', 'click wap', 'co free', 'code kl341', 'colour mobil', 'comp win', 'copi friend', 'cost 150pday', 'credit click', 'cri enough', 'csh11 send', 'cup final', 'custom select', 'da stock', 'date sunday', 'dont miss', 'dont think', 'dont want', 'eg england', 'eh rememb', 'england 87077', 'england macedonia', 'enough today', 'entitl updat', 'entri questionstd', 'entri wkli', 'even brother', 'fa 87121', 'fa cup', 'feel way', 'ffffffffff alright', 'final tkt', 'fine way', 'finish lunch', 'finish ur', 'first lar', 'free 08002986030', 'free call', 'free entri', 'free membership', 'friend callertun', 'go str', 'go tri', 'goalsteam news', 'goe usf', 'gonna home', 'ha ha', 'ha joke', 'hl info', 'home soon', 'httpwap xxxmobilemovieclubcomnqjkgighjjgcbl', 'im gonna', 'ive cri', 'jackpot txt', 'kim watch', 'kl341 valid', 'lar da', 'latest colour', 'lccltd pobox', 'like aid', 'like speak', 'link next', 'live around', 'lor ard', 'lor finish', 'lunch alreadi', 'lunch go', 'macedonia dont', 'make wet', 'may 2005', 'meet sooner', 'mell mell', 'mell oru', 'membership 100000', 'messag click', 'minnaminungint nurungu', 'miss goalsteam', 'mobil 11', 'mobil camera', 'mobil updat', 'month entitl', 'month ha', 'nah dont', 'name ye', 'nation team', 'naughti make', 'network custom', 'news txt', 'next txt', 'nurungu vettam', 'oh kim', 'oru minnaminungint', 'pay first', 'per request', 'pobox 4403ldnw1a7rw18', 'poboxox36504w45wq 16', 'pound txt', 'press copi', 'prize jackpot', 'prize reward', 'questionstd txt', 'ratetc appli', 'receiv entri', 'receivea 900', 'rememb spell', 'repli hl', 'request mell', 'reward claim', 'scotland 4txtú120', 'select receivea', 'send 87575', 'serious spell', 'set callertun', 'six chanc', 'smth lor', 'soon dont', 'speak treat', 'spell name', 'stock comin', 'str lor', 'stuff anymor', 'talk stuff', 'tc wwwdbuknet', 'team 87077', 'text fa', 'think goe', 'tkt 21st', 'tonight ive', 'treat like', 'tri month', 'trywal scotland', 'tsandc appli', 'txt csh11', 'txt messag', 'txt ratetc', 'txt ur', 'txt word', 'updat co', 'updat latest', 'ur lunch', 'ur nation', 'urgent week', 'use credit', 'usf live', 'valid 12', 'valu network', 'vettam set', 'want talk', 'wap link', 'way feel', 'way gota', 'way meet', 'week free', 'win cash', 'win fa', 'winner valu', 'wkli comp', 'word claim', 'wwwdbuknet lccltd', 'xxxmobilemovieclub use', 'ye naughti']\n"
     ]
    }
   ],
   "source": [
    "data_sample = data[0:20]\n",
    "ngram_vect_sample = CountVectorizer(ngram_range = (2,2))\n",
    "x_count_sample = ngram_vect_sample.fit_transform(data_sample['cleaned_text'])\n",
    "print(x_count_sample.shape)\n",
    "print(ngram_vect_sample.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>09061701461 claim</th>\n",
       "      <th>100 20000</th>\n",
       "      <th>100000 prize</th>\n",
       "      <th>11 month</th>\n",
       "      <th>12 hour</th>\n",
       "      <th>150pday 6day</th>\n",
       "      <th>16 tsandc</th>\n",
       "      <th>20000 pound</th>\n",
       "      <th>2005 text</th>\n",
       "      <th>21st may</th>\n",
       "      <th>...</th>\n",
       "      <th>way meet</th>\n",
       "      <th>week free</th>\n",
       "      <th>win cash</th>\n",
       "      <th>win fa</th>\n",
       "      <th>winner valu</th>\n",
       "      <th>wkli comp</th>\n",
       "      <th>word claim</th>\n",
       "      <th>wwwdbuknet lccltd</th>\n",
       "      <th>xxxmobilemovieclub use</th>\n",
       "      <th>ye naughti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   09061701461 claim  100 20000  100000 prize  11 month  12 hour  \\\n",
       "0                  0          0             0         0        0   \n",
       "1                  0          0             0         0        0   \n",
       "2                  0          0             0         0        0   \n",
       "3                  0          0             0         0        0   \n",
       "4                  0          0             0         0        0   \n",
       "\n",
       "   150pday 6day  16 tsandc  20000 pound  2005 text  21st may  ...  way meet  \\\n",
       "0             0          0            0          1         1  ...         0   \n",
       "1             0          0            0          0         0  ...         0   \n",
       "2             0          0            0          0         0  ...         0   \n",
       "3             0          0            0          0         0  ...         0   \n",
       "4             0          0            0          0         0  ...         0   \n",
       "\n",
       "   week free  win cash  win fa  winner valu  wkli comp  word claim  \\\n",
       "0          0         0       1            0          1           0   \n",
       "1          0         0       0            0          0           0   \n",
       "2          0         0       0            0          0           0   \n",
       "3          0         0       0            0          0           0   \n",
       "4          0         0       0            0          0           0   \n",
       "\n",
       "   wwwdbuknet lccltd  xxxmobilemovieclub use  ye naughti  \n",
       "0                  0                       0           0  \n",
       "1                  0                       0           0  \n",
       "2                  0                       0           0  \n",
       "3                  0                       0           0  \n",
       "4                  0                       0           0  \n",
       "\n",
       "[5 rows x 198 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_count_df = pd.DataFrame(x_count_sample.toarray())\n",
    "x_count_df.columns = ngram_vect_sample.get_feature_names()\n",
    "x_count_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing Raw Data: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Creates a document-term matrix where the columns represent single unique terms (unigrams) but the cell represents a weighting meant to represent how important a word is to a document.\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv(\"SMSSpamCollection.tsv\", sep='\\t')\n",
    "data.columns = ['label', 'body_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create function to remove punctuation, tokenize, remove stopwords, and stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5567, 8104)\n",
      "['', '0', '008704050406', '0089mi', '0121', '01223585236', '01223585334', '0125698789', '02', '020603', '0207', '02070836089', '02072069400', '02073162414', '02085076972', '020903', '021', '050703', '0578', '06', '060505', '061104', '07008009200', '07046744435', '07090201529', '07090298926', '07099833605', '071104', '07123456789', '0721072', '07732584351', '07734396839', '07742676969', '07753741225', '0776xxxxxxx', '07786200117', '077xxx', '078', '07801543489', '07808', '07808247860', '07808726822', '07815296484', '07821230901', '0784987', '0789xxxxxxx', '0794674629107880867867', '0796xxxxxx', '07973788240', '07xxxxxxxxx', '0800', '08000407165', '08000776320', '08000839402', '08000930705', '08000938767', '08001950382', '08002888812', '08002986030', '08002986906', '08002988890', '08006344447', '0808', '08081263000', '08081560665', '0825', '0844', '08448350055', '08448714184', '0845', '08450542832', '08452810071', '08452810073', '08452810075over18', '0870', '08700621170150p', '08701213186', '08701237397', '08701417012', '08701417012150p', '0870141701216', '087016248', '08701752560', '087018728737', '0870241182716', '08702490080', '08702840625', '08702840625comuk', '08704439680', '08704439680tsc', '08706091795', '0870737910216yr', '08707500020', '08707509020', '0870753331018', '08707808226', '08708034412', '08708800282', '08709222922', '08709501522']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfid_vect = TfidfVectorizer(analyzer = clean_text)\n",
    "x_tfidf = tfid_vect.fit_transform(data['body_text'])\n",
    "print(x_tfidf.shape)\n",
    "print(tfid_vect.get_feature_names()[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 192)\n",
      "['08002986030', '08452810075over18', '09061701461', '1', '100', '100000', '11', '12', '150pday', '16', '2', '20000', '2005', '21st', '3', '4', '4403ldnw1a7rw18', '4txtú120', '6day', '81010', '87077', '87121', '87575', '9', '900', 'aft', 'aid', 'alreadi', 'alright', 'anymor', 'appli', 'ard', 'around', 'b', 'brother', 'call', 'caller', 'callertun', 'camera', 'cash', 'chanc', 'claim', 'click', 'co', 'code', 'colour', 'comin', 'comp', 'copi', 'cost', 'credit', 'cri', 'csh11', 'cup', 'custom', 'da', 'date', 'dont', 'eg', 'eh', 'england', 'enough', 'entitl', 'entri', 'even', 'fa', 'feel', 'ffffffffff', 'final', 'fine', 'finish', 'first', 'free', 'friend', 'go', 'goalsteam', 'goe', 'gonna', 'gota', 'ha', 'hl', 'home', 'hour', 'httpwap', 'im', 'info', 'ive', 'jackpot', 'joke', 'k', 'kim', 'kl341', 'lar', 'latest', 'lccltd', 'like', 'link', 'live', 'lor', 'lunch']\n"
     ]
    }
   ],
   "source": [
    "data_sample = data[0:20]\n",
    "tfid_vect_sample = TfidfVectorizer(analyzer = clean_text)\n",
    "x_tfidf_sample = tfid_vect_sample.fit_transform(data_sample['body_text'])\n",
    "print(x_tfidf_sample.shape)\n",
    "print(tfid_vect_sample.get_feature_names()[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>08002986030</th>\n",
       "      <th>08452810075over18</th>\n",
       "      <th>09061701461</th>\n",
       "      <th>1</th>\n",
       "      <th>100</th>\n",
       "      <th>100000</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>150pday</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>wet</th>\n",
       "      <th>win</th>\n",
       "      <th>winner</th>\n",
       "      <th>wkli</th>\n",
       "      <th>word</th>\n",
       "      <th>wwwdbuknet</th>\n",
       "      <th>xxxmobilemovieclub</th>\n",
       "      <th>xxxmobilemovieclubcomnqjkgighjjgcbl</th>\n",
       "      <th>ye</th>\n",
       "      <th>ü</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.198986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.174912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.198986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 192 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   08002986030  08452810075over18  09061701461    1  100  100000   11   12  \\\n",
       "0          0.0           0.198986          0.0  0.0  0.0     0.0  0.0  0.0   \n",
       "1          0.0           0.000000          0.0  0.0  0.0     0.0  0.0  0.0   \n",
       "2          0.0           0.000000          0.0  0.0  0.0     0.0  0.0  0.0   \n",
       "3          0.0           0.000000          0.0  0.0  0.0     0.0  0.0  0.0   \n",
       "4          0.0           0.000000          0.0  0.0  0.0     0.0  0.0  0.0   \n",
       "\n",
       "   150pday   16  ...  wet       win  winner      wkli  word  wwwdbuknet  \\\n",
       "0      0.0  0.0  ...  0.0  0.174912     0.0  0.198986   0.0         0.0   \n",
       "1      0.0  0.0  ...  0.0  0.000000     0.0  0.000000   0.0         0.0   \n",
       "2      0.0  0.0  ...  0.0  0.000000     0.0  0.000000   0.0         0.0   \n",
       "3      0.0  0.0  ...  0.0  0.000000     0.0  0.000000   0.0         0.0   \n",
       "4      0.0  0.0  ...  0.0  0.000000     0.0  0.000000   0.0         0.0   \n",
       "\n",
       "   xxxmobilemovieclub  xxxmobilemovieclubcomnqjkgighjjgcbl   ye    ü  \n",
       "0                 0.0                                  0.0  0.0  0.0  \n",
       "1                 0.0                                  0.0  0.0  0.0  \n",
       "2                 0.0                                  0.0  0.0  0.0  \n",
       "3                 0.0                                  0.0  0.0  0.0  \n",
       "4                 0.0                                  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 192 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tfid_df = pd.DataFrame(x_tfidf_sample.toarray())\n",
    "x_tfid_df.columns = tfid_vect_sample.get_feature_names()\n",
    "x_tfid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
